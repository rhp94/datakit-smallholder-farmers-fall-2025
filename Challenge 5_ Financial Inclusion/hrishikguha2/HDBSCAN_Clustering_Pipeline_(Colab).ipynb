{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install polars sentence-transformers"
      ],
      "metadata": {
        "id": "q1p6_BBN7A8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- Installing RAPIDS cuML for GPU Acceleration ---\")\n",
        "!pip install cudf-cu12 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com"
      ],
      "metadata": {
        "id": "3itFCvcj_-83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- COLAB SETUP ---\n",
        "# Install the required libraries in the Colab environment\n",
        "#!pip install polars sentence-transformers hdbscan scikit-learn\n",
        "\n",
        "import polars as pl\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import HDBSCAN\n",
        "import hdbscan # Explicitly import hdbscan if needed, though sklearn.cluster is used for the HDBSCAN model\n",
        "\n",
        "# --- Import RAPIDS Libraries ---\n",
        "import cudf\n",
        "from cuml.cluster import HDBSCAN as cuML_HDBSCAN\n",
        "# We will rename the cuML HDBSCAN to avoid conflicts\n",
        "# --- CONFIGURATION PATHS (Colab Edition) ---\n",
        "\n",
        "# IMPORTANT: You must upload the file 'african_questions_TRANSLATED_SAVED.parquet'\n",
        "# to your Colab session. The path below assumes the file is in the root directory.\n",
        "INPUT_FILE_TRANSLATED = 'african_questions_TRANSLATED_SAVED.parquet'\n",
        "\n",
        "# Output directory for the final clustered file\n",
        "OUTPUT_FOLDER = './translated_data'\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "OUTPUT_FILE_CLUSTERED = os.path.join(OUTPUT_FOLDER, 'african_questions_CLUSTERED.parquet')\n",
        "\n",
        "# --- PROCESSING PARAMETERS ---\n",
        "# HDBSCAN parameters set for robustness and meaningful clusters\n",
        "MIN_CLUSTER_SIZE = 20 # Minimum size of a cluster (recommended for initial stability)\n",
        "MIN_SAMPLES = 5       # How conservative the clustering should be\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "print(\"--- STAGE: Thematic Clustering ---\")\n",
        "\n",
        "if not os.path.exists(INPUT_FILE_TRANSLATED):\n",
        "    print(f\"FATAL: Translated data not found at {INPUT_FILE_TRANSLATED}.\")\n",
        "    print(\"Please ensure you have uploaded 'african_questions_TRANSLATED_SAVED.parquet' to this Colab environment.\")\n",
        "else:\n",
        "    # 1. Load the complete translated dataset\n",
        "    print(f\"Loading complete translated dataset from: {INPUT_FILE_TRANSLATED}\")\n",
        "    # We only need the English content column and any other existing columns (like detected_language)\n",
        "    df_translated = pl.read_parquet(INPUT_FILE_TRANSLATED)\n",
        "\n",
        "    # 2. Initialize the Sentence Transformer model for generating semantic embeddings\n",
        "    # 'all-MiniLM-L6-v2' provides a good balance of speed and semantic quality.\n",
        "    embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    print(\"Sentence Transformer Model loaded for embedding.\")\n",
        "\n",
        "    # 3. Generate Semantic Embeddings\n",
        "    print(\"Generating Semantic Embeddings...\")\n",
        "    # Ensure we are using the English column for embedding\n",
        "    questions_en = df_translated['question_content_en'].to_list()\n",
        "    # The progress bar is useful for tracking this computationally heavy step\n",
        "    embeddings_np = embedder.encode(questions_en, show_progress_bar=True)\n",
        "\n",
        "\n",
        "# CRITICAL STEP: Convert NumPy array to cuDF DataFrame for GPU processing\n",
        "    embeddings_gpu = cudf.DataFrame(embeddings_np)\n",
        "    print(f\"Embeddings converted to GPU-accelerated cuDF DataFrame ({embeddings_gpu.shape}).\")\n",
        "\n",
        "\n",
        "    # 4. Perform HDBSCAN Clustering using cuML (GPU)\n",
        "    print(f\"Performing HDBSCAN Clustering using cuML (min_cluster_size={MIN_CLUSTER_SIZE})...\")\n",
        "\n",
        "    # Initialize the cuML HDBSCAN model\n",
        "    clusterer_gpu = cuML_HDBSCAN(\n",
        "        min_cluster_size=MIN_CLUSTER_SIZE,\n",
        "        min_samples=MIN_SAMPLES,\n",
        "        metric='euclidean' # cuML is highly optimized for this distance metric\n",
        "        # No need for n_jobs=-1, as cuML uses the entire GPU by default.\n",
        "    )\n",
        "\n",
        "    # Fit the clusterer and get the cluster assignments (IDs)\n",
        "    clusterer_gpu.fit(embeddings_gpu)\n",
        "    cluster_ids_gpu = clusterer_gpu.labels_.to_numpy() # Convert GPU results back to NumPy\n",
        "\n",
        "\n",
        "    # 5. Add the new cluster ID column to the DataFrame\n",
        "    df_translated = df_translated.with_columns(\n",
        "        pl.Series(name=\"cluster_id\", values=cluster_ids_gpu)\n",
        "    )\n",
        "\n",
        "\n",
        "    # 6. Save the results\n",
        "    df_translated.write_parquet(OUTPUT_FILE_CLUSTERED, compression='zstd')\n",
        "    print(f\"\\n✅ GPU Clustering complete. Results saved to: {OUTPUT_FILE_CLUSTERED}\")\n",
        "\n",
        "    # --- Final Output Summary ---\n",
        "    print(\"\\n--- Summary of Top 5 Largest Clusters (Thematic Needs) ---\")\n",
        "    # Cluster ID -1 represents noise (unclassifiable questions)\n",
        "    cluster_counts = df_translated.filter(pl.col(\"cluster_id\") != -1) \\\n",
        "                                  .group_by('cluster_id') \\\n",
        "                                  .count() \\\n",
        "                                  .sort('count', descending=True) \\\n",
        "                                  .head(5)\n",
        "\n",
        "    print(cluster_counts)\n",
        "\n",
        "    # Next step reminder\n",
        "    print(\"\\nNext step: Run the 'financial_intent_mapper.py' (in memory) to analyze these clusters.\")\n",
        "\n",
        "    print(cluster_counts)\n",
        "\n",
        "    # Next step reminder\n",
        "    print(\"\\nNext step: Run the 'financial_intent_mapper.py' (in memory) to analyze these clusters.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "nxMKKCd8ZL3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CONFIGURATION ---\n",
        "CLUSTERED_DATA_PATH = './translated_data/african_questions_CLUSTERED.parquet'\n",
        "OUTPUT_TAXONOMY_PATH = './financial_taxonomy_SYNC.json'\n",
        "SAMPLE_SIZE_PER_CLUSTER = 10\n",
        "# *** USER REQUESTED CHANGE: Increased minimum cluster size ***\n",
        "MIN_CLUSTER_COUNT_FOR_SEPARATE_ANALYSIS = 200\n",
        "VIRTUAL_CLUSTER_ID = -999\n",
        "\n",
        "# --- IMPORTANT: API KEY (Single key for synchronous requests) ---\n",
        "# We will rely only on the first provided key from the environment variable.\n",
        "API_KEY = os.environ.get(\"GEMINI_API_KEY\", \"AIzaSyDLkx8f7CAkx7Sw1tcPt8v3B_nbl2LVxsI\") # <-- REPLACE THIS STRING\n",
        "\n",
        "MODEL_NAME = \"gemini-2.5-flash-preview-09-2025\"\n",
        "# *** ENDPOINT MODIFIED BACK TO SINGLE generateContent ***\n",
        "BASE_API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/{MODEL_NAME}:generateContent?key=\"\n",
        "\n",
        "# --- LLM UTILITY FUNCTIONS ---\n",
        "\n",
        "def create_single_request_payload(cluster_id, sample_questions):\n",
        "    \"\"\"\n",
        "    Creates the JSON payload for a single cluster analysis.\n",
        "    \"\"\"\n",
        "    system_prompt = (\n",
        "        \"You are a Senior Financial Product Analyst for a non-profit serving smallholder farmers. \"\n",
        "        \"Your task is to analyze a sample of farmer questions from a single thematic cluster and assign a primary financial need \"\n",
        "        \"based on the underlying problem. Use only the provided JSON schema for output.\"\n",
        "    )\n",
        "\n",
        "    if cluster_id == VIRTUAL_CLUSTER_ID:\n",
        "        cluster_description = \"from a combined batch of many small, diverse clusters\"\n",
        "        need_instruction = (\n",
        "            \"Determine the single, most common financial need and product idea that represents the DIVERSITY of the questions, \"\n",
        "            \"or assign 'None/Information Need' if the questions are too varied.\"\n",
        "        )\n",
        "    else:\n",
        "        cluster_description = f\"from Cluster ID {cluster_id}\"\n",
        "        need_instruction = \"Determine the single, most relevant financial product category, the underlying financial need, and a simple product idea.\"\n",
        "\n",
        "    sample_text = \"\\n- \" + \"\\n- \".join(sample_questions)\n",
        "    user_query = (\n",
        "        f\"Analyze the following {len(sample_questions)} translated farmer questions {cluster_description}. \"\n",
        "        f\"{need_instruction} \"\n",
        "        \"Financial Need categories must ONLY be one of: Working Capital, Asset Financing, Risk Mitigation/Insurance, Liquidity/Savings, Market Access, or None/Information Need. \"\n",
        "        f\"Questions:\\n{sample_text}\"\n",
        "    )\n",
        "\n",
        "    response_schema = {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"clusterId\": {\"type\": \"INTEGER\", \"description\": \"The input cluster ID.\"},\n",
        "            \"financialNeed\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"The core financial problem. Choose one of: Working Capital, Asset Financing, Risk Mitigation/Insurance, Liquidity/Savings, Market Access, or None/Information Need.\"\n",
        "            },\n",
        "            \"productOpportunity\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"A specific, concise financial product idea to solve this need (e.g., 'Emergency Micro-loan for inputs', 'Crop-failure insurance').\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return {\n",
        "        \"contents\": [{\"parts\": [{\"text\": user_query}]}],\n",
        "        \"systemInstruction\": {\"parts\": [{\"text\": system_prompt}]},\n",
        "        \"generationConfig\": {\n",
        "            \"responseMimeType\": \"application/json\",\n",
        "            \"responseSchema\": response_schema\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def call_gemini_api_with_retry(payload, api_key, max_retries=5):\n",
        "    \"\"\"\n",
        "    Makes a single API call with exponential backoff for rate limit handling.\n",
        "    \"\"\"\n",
        "    api_url = f\"{BASE_API_URL}{api_key}\"\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(api_url, headers={'Content-Type': 'application/json'}, json=payload)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            raw_response = response.json()\n",
        "\n",
        "            if raw_response.get('candidates'):\n",
        "                json_string = raw_response['candidates'][0]['content']['parts'][0].get('text', '{}')\n",
        "\n",
        "                # Handle common markdown wrapping issue\n",
        "                if json_string.strip().startswith(\"```json\"):\n",
        "                    json_string = json_string.strip()[7:-3].strip()\n",
        "\n",
        "                parsed_result = json.loads(json_string)\n",
        "                return parsed_result\n",
        "            else:\n",
        "                raise ValueError(\"Model returned no candidates or empty content.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            status_code = response.status_code if response is not None else 0\n",
        "\n",
        "            if status_code == 429 or status_code == 500 or status_code == 503:\n",
        "                wait_time = 10 * (2 ** attempt) + random.uniform(0, 5) # Added jitter\n",
        "                print(f\"\\n[API ERROR {status_code}] Rate limit hit or transient error. Waiting {wait_time:.2f}s before retrying...\")\n",
        "                time.sleep(wait_time)\n",
        "                continue\n",
        "\n",
        "            if status_code in [400, 403]:\n",
        "                print(f\"\\n[CRITICAL API ERROR {status_code}] Check API key, permissions, or payload structure.\")\n",
        "                raise Exception(f\"Failed to connect: {status_code}. Aborting.\")\n",
        "\n",
        "            print(f\"[Failure] API Call failed (Attempt {attempt+1}/{max_retries}): {e}. Retrying...\")\n",
        "            time.sleep(2 ** attempt)\n",
        "\n",
        "    raise Exception(f\"Failed to get successful API response after {max_retries} retries for cluster.\")\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    !pip install polars\n",
        "\n",
        "    if not API_KEY or API_KEY.startswith(\"YOUR_\"):\n",
        "        print(\"\\n--- WARNING: API Key Not Provided ---\")\n",
        "        print(\"Please replace the placeholder string in the API_KEY variable with your actual Gemini API key.\")\n",
        "        exit(1)\n",
        "\n",
        "    print(f\"Starting synchronous analysis using one API key ({API_KEY[-4:]})...\")\n",
        "    print(f\"Minimum cluster size for separate analysis set to: {MIN_CLUSTER_COUNT_FOR_SEPARATE_ANALYSIS}\")\n",
        "\n",
        "\n",
        "    if not os.path.exists(CLUSTERED_DATA_PATH):\n",
        "        print(f\"FATAL: Clustered data not found at {CLUSTERED_DATA_PATH}. Please ensure the previous step was completed successfully.\")\n",
        "        exit(1)\n",
        "    else:\n",
        "        print(\"Loading clustered data for Synchronous Financial Intent Mapping...\")\n",
        "        df = pl.read_parquet(CLUSTERED_DATA_PATH, columns=['question_content_en', 'cluster_id'])\n",
        "\n",
        "        # 1. Filter out noise and count cluster sizes\n",
        "        df_clusters = df.filter(pl.col(\"cluster_id\") != -1)\n",
        "        cluster_counts = df_clusters.group_by('cluster_id').count().sort(\"cluster_id\")\n",
        "\n",
        "        # 2. Separate large clusters from small clusters\n",
        "        large_cluster_counts_df = cluster_counts.filter(pl.col(\"count\") >= MIN_CLUSTER_COUNT_FOR_SEPARATE_ANALYSIS)\n",
        "        large_cluster_ids = large_cluster_counts_df['cluster_id'].to_list()\n",
        "        small_cluster_ids = cluster_counts.filter(pl.col(\"count\") < MIN_CLUSTER_COUNT_FOR_SEPARATE_ANALYSIS)['cluster_id'].to_list()\n",
        "\n",
        "        # 3. Identify all clusters to analyze\n",
        "        # Use a slice to ensure we are working with a fresh list for appending\n",
        "        clusters_to_analyze = large_cluster_ids[:]\n",
        "        if small_cluster_ids:\n",
        "            clusters_to_analyze.append(VIRTUAL_CLUSTER_ID)\n",
        "\n",
        "        # --- Sanity check added to diagnose the skipped cluster ---\n",
        "        print(f\"Total clusters to process: {len(clusters_to_analyze)} out of {df_clusters.select(pl.col('cluster_id').unique().count()).item()} total clusters.\")\n",
        "        print(f\"Confirmed clusters to process: {clusters_to_analyze}\")\n",
        "\n",
        "        # 4. Prepare cluster samples\n",
        "        cluster_samples = {}\n",
        "        for row in tqdm(large_cluster_counts_df.rows(named=True), total=len(large_cluster_ids), desc=\"Sampling Large Clusters\"):\n",
        "            cluster_id = row['cluster_id']\n",
        "            cluster_size = row['count']\n",
        "            n_sample = min(SAMPLE_SIZE_PER_CLUSTER, cluster_size)\n",
        "            sample_df = df_clusters.filter(pl.col(\"cluster_id\") == cluster_id).sample(n=n_sample, with_replacement=False, seed=42)\n",
        "            cluster_samples[cluster_id] = sample_df['question_content_en'].to_list()\n",
        "\n",
        "        if small_cluster_ids:\n",
        "            df_small_clusters = df_clusters.filter(pl.col(\"cluster_id\").is_in(small_cluster_ids))\n",
        "            total_small_cluster_size = df_small_clusters.height\n",
        "            n_sample_virtual = min(SAMPLE_SIZE_PER_CLUSTER, total_small_cluster_size)\n",
        "            sample_df_virtual = df_small_clusters.sample(n=n_sample_virtual, with_replacement=False, seed=42)\n",
        "            cluster_samples[VIRTUAL_CLUSTER_ID] = sample_df_virtual['question_content_en'].to_list()\n",
        "\n",
        "\n",
        "        # 5. Perform the Synchronous LLM Analysis\n",
        "        taxonomy_results = []\n",
        "        success_count = 0\n",
        "        error_count = 0\n",
        "\n",
        "        print(f\"\\nStarting Synchronous Classification for {len(clusters_to_analyze)} clusters...\")\n",
        "\n",
        "        for cluster_id in tqdm(clusters_to_analyze, desc=\"Classifying Financial Intent\"):\n",
        "            # Check if the cluster was actually sampled (prevents the 'Warning' from breaking execution)\n",
        "            if cluster_id not in cluster_samples:\n",
        "                 error_count += 1\n",
        "                 print(f\"\\n[SKIP ERROR] Cluster ID {cluster_id} was identified for processing but not found in samples. Skipping.\")\n",
        "                 taxonomy_results.append({\n",
        "                    \"clusterId\": cluster_id,\n",
        "                    \"financialNeed\": \"SETUP_ERROR\",\n",
        "                    \"productOpportunity\": \"Missing cluster sample due to setup error.\"\n",
        "                 })\n",
        "                 continue\n",
        "\n",
        "            sample_questions = cluster_samples[cluster_id]\n",
        "            request_payload = create_single_request_payload(cluster_id, sample_questions)\n",
        "\n",
        "            try:\n",
        "                result = call_gemini_api_with_retry(request_payload, API_KEY)\n",
        "\n",
        "                # Check if the result has the necessary fields and the correct cluster ID\n",
        "                if result and result.get('clusterId') == cluster_id:\n",
        "                    taxonomy_results.append(result)\n",
        "                    success_count += 1\n",
        "                else:\n",
        "                    error_count += 1\n",
        "                    taxonomy_results.append({\n",
        "                        \"clusterId\": cluster_id,\n",
        "                        \"financialNeed\": \"VALIDATION_ERROR\",\n",
        "                        \"productOpportunity\": \"Parsed JSON missing required fields or incorrect ID.\"\n",
        "                    })\n",
        "\n",
        "            except Exception as e:\n",
        "                error_count += 1\n",
        "                taxonomy_results.append({\n",
        "                    \"clusterId\": cluster_id,\n",
        "                    \"financialNeed\": \"API_FAILURE\",\n",
        "                    \"productOpportunity\": f\"Synchronous API call failed: {str(e)}\"\n",
        "                })\n",
        "\n",
        "\n",
        "        # 6. Save the final results\n",
        "        if taxonomy_results:\n",
        "            os.makedirs(os.path.dirname(OUTPUT_TAXONOMY_PATH), exist_ok=True)\n",
        "            with open(OUTPUT_TAXONOMY_PATH, 'w') as f:\n",
        "                json.dump(taxonomy_results, f, indent=4)\n",
        "\n",
        "            print(f\"\\n✅ Synchronous Financial Taxonomy Attempt Complete! Results saved to: {OUTPUT_TAXONOMY_PATH}\")\n",
        "            print(f\"   Successes: {success_count}, Failures: {error_count}\")\n",
        "\n",
        "            # 7. Print a summary of the top financial needs\n",
        "            needs_list = [res['financialNeed'] for res in taxonomy_results if res['financialNeed'] not in ['API_FAILURE', 'VALIDATION_ERROR', 'SETUP_ERROR']]\n",
        "            needs_count = Counter(needs_list).most_common()\n",
        "\n",
        "            print(\"\\n--- Summary of Top Financial Needs Mapped to Clusters ---\")\n",
        "            for need, count in needs_count:\n",
        "                print(f\"- {need}: {count} Clusters\")\n",
        "\n",
        "        print(\"\\n**CRITICAL ADVICE:**\")\n",
        "        print(\"This synchronous approach is highly vulnerable to rate limits and will be slow.\")\n",
        "        print(\"To guarantee completion, please use the provided **offline_financial_intent_mapper.py** file.\")"
      ],
      "metadata": {
        "id": "h99mLjvHjPDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Strategic Takeaway for Producers Direct\n",
        "The data divides the user base into two distinct groups based on their need frequency:\n",
        "\n",
        "The Information Majority (The Bulk): Focus on information-delivery channels. These users need rapid, accurate, and localized advice.\n",
        "\n",
        "The Financial Minority (The High-Value Opportunity): Focus financial product development on the specific themes represented by the 5 financially-classified clusters. Do not try to solve the financial need of the entire user base; target the small, specific, high-intent groups."
      ],
      "metadata": {
        "id": "1h-68Ik2moAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Possible strategic implications of financial need category count*\n",
        "\n",
        "**None/Information Needed:11 Clusters**\tCore Value Proposition: The most frequent and massive demand from farmers is for **advice, knowledge, and technical guidance** (e.g., pest control, planting methods, soil health). This confirms that scaling the quality and speed of technical response is the organization's current core mandate based on user behavior.\n",
        "\n",
        "**Working Capital: 2 Clusters**\tHigh-Demand, Transactional Need: These clusters represent **immediate operational needs**, likely for inputs like seeds and fertilizer. This is an immediate, high-priority product opportunity for a **short-term micro-loan or credit facility**.\n",
        "\n",
        "**Risk Mitigation/Insurance:\t2 Clusters**\tProtection Need: These clusters reflect **farmer concerns about external threats** (weather, pests, disease). This signals a strong opportunity for developing **targeted, parametric insurance products **(like weather-indexed insurance).\n",
        "\n",
        "**Asset Financing:\t1 Cluster**\tGrowth Need: This cluster likely relates to **needs for high-value equipment** like solar pumps, storage facilities, or machinery. This points toward opportunities for **group loans or lease-to-own programs designed for capital investment and long-term farm growth.**"
      ],
      "metadata": {
        "id": "hfctUFAam825"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- CONFIGURATION ---\n",
        "CLUSTERED_DATA_PATH = './translated_data/african_questions_CLUSTERED.parquet'\n",
        "TAXONOMY_PATH = './financial_taxonomy_SYNC.json' # Using the output from the last successful step\n",
        "OUTPUT_DF_PATH = './analysis_data/farmer_questions_FOR_ANALYSIS.parquet'\n",
        "VIRTUAL_CLUSTER_ID = -999\n",
        "\n",
        "# --- SEVERITY MAPPING ---\n",
        "# Define the weight for each financial need in the Severity calculation (0.0 to 1.0)\n",
        "# Financial needs are given higher severity scores (S-scores) than information needs (I-scores).\n",
        "# The Final Severity Score = Cluster Frequency * Severity Weight\n",
        "SEVERITY_WEIGHTS = {\n",
        "    \"Working Capital\": 0.9,       # High urgency (inputs needed NOW)\n",
        "    \"Risk Mitigation/Insurance\": 0.8, # High importance (livelihood protection)\n",
        "    \"Asset Financing\": 0.7,       # Medium importance (long-term growth)\n",
        "    \"Liquidity/Savings\": 0.5,     # Medium importance (financial resilience)\n",
        "    \"Market Access\": 0.4,         # Lower importance (often informational, but affects income)\n",
        "    \"None/Information Need\": 0.2, # Lowest urgency/financial impact\n",
        "    \"API_FAILURE\": 0.0,           # Set failure clusters to zero weight\n",
        "    \"VALIDATION_ERROR\": 0.0,\n",
        "    \"SETUP_ERROR\": 0.0\n",
        "}\n",
        "\n",
        "\n",
        "def calculate_cluster_metrics():\n",
        "    \"\"\"\n",
        "    Loads the clustered data, merges the financial taxonomy, calculates cluster\n",
        "    frequency, and assigns a base severity score.\n",
        "    \"\"\"\n",
        "    print(\"Starting Severity Analysis Preprocessing...\")\n",
        "\n",
        "    # 1. Load DataFrames\n",
        "    if not os.path.exists(CLUSTERED_DATA_PATH):\n",
        "        print(f\"FATAL: Clustered data not found at {CLUSTERED_DATA_PATH}. Aborting.\")\n",
        "        return\n",
        "    df_questions = pl.read_parquet(CLUSTERED_DATA_PATH)\n",
        "\n",
        "    if not os.path.exists(TAXONOMY_PATH):\n",
        "        print(f\"FATAL: Financial taxonomy not found at {TAXONOMY_PATH}. Aborting.\")\n",
        "        return\n",
        "    with open(TAXONOMY_PATH, 'r') as f:\n",
        "        taxonomy_data = json.load(f)\n",
        "\n",
        "    # 2. Convert taxonomy list to Polars DataFrame (though not strictly needed, good for inspection)\n",
        "    # taxonomy_df = pl.DataFrame(taxonomy_data) # Not used below, simplifying to dict lookup\n",
        "\n",
        "    # 3. Calculate Cluster Size and Frequency\n",
        "\n",
        "    # Filter out noise (-1) clusters for frequency analysis\n",
        "    df_filtered = df_questions.filter(pl.col(\"cluster_id\") != -1)\n",
        "\n",
        "    # Calculate the total number of non-noise questions\n",
        "    total_questions = df_filtered.height\n",
        "\n",
        "    # Group by cluster ID to get size (count)\n",
        "    cluster_size_df = df_filtered.group_by('cluster_id').count().rename({\"count\": \"cluster_size\"})\n",
        "\n",
        "    # Calculate frequency (size / total_questions)\n",
        "    cluster_size_df = cluster_size_df.with_columns(\n",
        "        (pl.col(\"cluster_size\") / total_questions).alias(\"cluster_frequency\")\n",
        "    )\n",
        "\n",
        "    # 4. Merge Taxonomy into Cluster Metrics\n",
        "\n",
        "    # Create a mapping dictionary from the taxonomy data\n",
        "    taxonomy_map = {item['clusterId']: item for item in taxonomy_data}\n",
        "\n",
        "    # Function to lookup financial data for each cluster\n",
        "    def get_financial_data(cluster_id):\n",
        "        # Handle the virtual cluster ID lookup specifically\n",
        "        lookup_id = VIRTUAL_CLUSTER_ID if cluster_id not in taxonomy_map else cluster_id\n",
        "\n",
        "        # If the cluster ID is large (>=200) or the Virtual Cluster, use its specific entry\n",
        "        if lookup_id in taxonomy_map:\n",
        "            return taxonomy_map[lookup_id]\n",
        "\n",
        "        # If the cluster ID is small (<200) but NOT the Virtual Cluster ID,\n",
        "        # it means it was a small cluster, so we use the Virtual Cluster's classification.\n",
        "        if cluster_id not in taxonomy_map and VIRTUAL_CLUSTER_ID in taxonomy_map:\n",
        "            return taxonomy_map[VIRTUAL_CLUSTER_ID]\n",
        "\n",
        "        # Default for unclassified clusters (should only happen if -999 failed)\n",
        "        return {\"financialNeed\": \"UNCATEGORIZED\", \"productOpportunity\": \"Data missing.\"}\n",
        "\n",
        "    # Apply the mapping logic to the cluster_size_df\n",
        "    # We apply the classification based on the logic:\n",
        "    # - If cluster is large, use its specific classification.\n",
        "    # - If cluster is small, use the Virtual Cluster's (-999) classification.\n",
        "    cluster_metrics = cluster_size_df.with_columns(\n",
        "        # FIX: Changed .apply() to .map_elements() for Polars compatibility\n",
        "        pl.col(\"cluster_id\").map_elements(lambda id: get_financial_data(id)['financialNeed'], return_dtype=pl.String).alias(\"financial_need\"),\n",
        "        pl.col(\"cluster_id\").map_elements(lambda id: get_financial_data(id)['productOpportunity'], return_dtype=pl.String).alias(\"product_opportunity\")\n",
        "    )\n",
        "\n",
        "    # 5. Assign Severity Weights\n",
        "    cluster_metrics = cluster_metrics.with_columns(\n",
        "        # FIX: Changed .apply() to .map_elements() for Polars compatibility\n",
        "        pl.col(\"financial_need\").map_elements(lambda need: SEVERITY_WEIGHTS.get(need, 0.0), return_dtype=pl.Float64).alias(\"severity_weight\")\n",
        "    )\n",
        "\n",
        "    # 6. Calculate Final Severity Score (Frequency * Severity Weight)\n",
        "    # This is the core calculation for prioritization: score = F * W\n",
        "    cluster_metrics = cluster_metrics.with_columns(\n",
        "        (pl.col(\"cluster_frequency\") * pl.col(\"severity_weight\")).alias(\"severity_score\")\n",
        "    )\n",
        "\n",
        "    # 7. Finalize and Save\n",
        "    # Sort descending to put the highest priority clusters at the top\n",
        "    final_df = cluster_metrics.sort(\"severity_score\", descending=True)\n",
        "\n",
        "    os.makedirs(os.path.dirname(OUTPUT_DF_PATH), exist_ok=True)\n",
        "    final_df.write_parquet(OUTPUT_DF_PATH)\n",
        "\n",
        "    print(f\"\\n✅ Severity Analysis Preprocessing Complete! Metrics saved to: {OUTPUT_DF_PATH}\")\n",
        "    print(\"\\n--- Top 5 Clusters by Severity Score ---\")\n",
        "    print(final_df.head(5))\n",
        "\n",
        "    print(\"\\nNext Steps:\")\n",
        "    print(\"1. Review the final sorted table to identify the highest priority clusters.\")\n",
        "    print(\"2. The next file will present the final strategic report and data visualization.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    !pip install polars\n",
        "    calculate_cluster_metrics()"
      ],
      "metadata": {
        "id": "2fIFBpD_p0QH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "INPUT_DF_PATH = './analysis_data/farmer_questions_FOR_ANALYSIS.parquet'\n",
        "REPORT_FILENAME = './analysis_data/top_priority_clusters_report.txt'\n",
        "PLOT_FILENAME = './analysis_data/severity_score_bar_chart.png'\n",
        "TOP_N_CLUSTERS = 10 # Number of top clusters to visualize and report\n",
        "\n",
        "\n",
        "def generate_strategic_report():\n",
        "    \"\"\"\n",
        "    Loads the scored data, generates a structured text report, and creates a\n",
        "    visualization of the top priority clusters for executive review.\n",
        "    \"\"\"\n",
        "    print(\"Starting Final Strategic Report Generation...\")\n",
        "\n",
        "    if not os.path.exists(INPUT_DF_PATH):\n",
        "        print(f\"FATAL: Input data for analysis not found at {INPUT_DF_PATH}. Please run the Preprocessor step first.\")\n",
        "        return\n",
        "\n",
        "    # 1. Load the final scored data\n",
        "    df = pl.read_parquet(INPUT_DF_PATH)\n",
        "\n",
        "    # Filter out clusters with zero severity (API failures, UNCATEGORIZED)\n",
        "    df_ranked = df.filter(pl.col(\"severity_score\") > 0).sort(\"severity_score\", descending=True)\n",
        "    df_top = df_ranked.head(TOP_N_CLUSTERS)\n",
        "\n",
        "    if df_top.is_empty():\n",
        "        print(\"WARNING: No clusters found with a severity score greater than zero.\")\n",
        "        return\n",
        "\n",
        "    # 2. Prepare the Text Report\n",
        "    report_content = []\n",
        "\n",
        "    report_content.append(\"=\"*80)\n",
        "    report_content.append(f\"PRODUCERS DIRECT: TOP {TOP_N_CLUSTERS} PRIORITIZED NEEDS ANALYSIS\")\n",
        "    report_content.append(\"=\"*80)\n",
        "    report_content.append(f\"Total Clusters Analyzed: {df.height} (excluding noise)\")\n",
        "    report_content.append(f\"Total Questions Analyzed: {df['cluster_size'].sum()}\")\n",
        "    report_content.append(\"\\nThis report ranks clusters by 'Severity Score' (Cluster Frequency * Financial Urgency Weight).\")\n",
        "    report_content.append(\"It identifies the most impactful problems requiring immediate strategic attention.\")\n",
        "    report_content.append(\"=\"*80)\n",
        "\n",
        "    for i, row in enumerate(df_top.rows(named=True)):\n",
        "        cluster_id = row['cluster_id']\n",
        "        size = row['cluster_size']\n",
        "        frequency = row['cluster_frequency'] * 100\n",
        "        severity_score = row['severity_score'] * 100 # Display score as a percentage for readability\n",
        "        need = row['financial_need']\n",
        "        opportunity = row['product_opportunity']\n",
        "\n",
        "        report_content.append(f\"\\nRANK #{i+1} | SCORE: {severity_score:.4f}% | Cluster ID: {cluster_id}\")\n",
        "        report_content.append(f\"  > Primary Need: {need}\")\n",
        "        report_content.append(f\"  > Size/Frequency: {size} questions ({frequency:.2f}% of total)\")\n",
        "\n",
        "        # Wrap the opportunity text for clean output\n",
        "        wrapped_opportunity = textwrap.fill(\n",
        "            f\"  > Strategic Opportunity: {opportunity}\",\n",
        "            width=75,\n",
        "            initial_indent=\"    \",\n",
        "            subsequent_indent=\"    \"\n",
        "        )\n",
        "        report_content.append(wrapped_opportunity)\n",
        "        report_content.append(\"-\" * 40)\n",
        "\n",
        "    # Save the text report\n",
        "    os.makedirs(os.path.dirname(REPORT_FILENAME), exist_ok=True)\n",
        "    with open(REPORT_FILENAME, 'w') as f:\n",
        "        f.write('\\n'.join(report_content))\n",
        "    print(f\"\\n✅ Strategic Text Report saved to: {REPORT_FILENAME}\")\n",
        "\n",
        "    # 3. Generate Visualization\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    plot_data = df_top.sort(\"severity_score\", descending=False) # Sort ascending for correct bar chart order\n",
        "\n",
        "    scores = plot_data['severity_score'].to_list()\n",
        "    # Create labels combining ID and Need, and wrap long text\n",
        "    labels = [\n",
        "        f\"ID {row['cluster_id']} ({row['financial_need']})\"\n",
        "        for row in plot_data.rows(named=True)\n",
        "    ]\n",
        "\n",
        "    # Define colors based on need type (Financial vs. Information)\n",
        "    colors = ['#1f77b4' if 'None/Information' not in n else '#ff7f0e' for n in plot_data['financial_need'].to_list()]\n",
        "\n",
        "    plt.style.use('ggplot')\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    # Create the horizontal bar chart\n",
        "    ax.barh(labels, scores, color=colors)\n",
        "\n",
        "    ax.set_title(f\"Top {TOP_N_CLUSTERS} Priority Clusters Ranked by Severity Score\", fontsize=14)\n",
        "    ax.set_xlabel(\"Severity Score (Frequency × Weight)\", fontsize=12)\n",
        "    ax.set_ylabel(\"Cluster ID and Primary Need\", fontsize=12)\n",
        "\n",
        "    # Add a legend\n",
        "    from matplotlib.lines import Line2D\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='#1f77b4', lw=4, label='Direct Financial Need'),\n",
        "        Line2D([0], [0], color='#ff7f0e', lw=4, label='Information Need')\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='lower right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(PLOT_FILENAME)\n",
        "    print(f\"✅ Visualization saved to: {PLOT_FILENAME}\")\n",
        "\n",
        "    print(\"\\nFinal Analysis Complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure Polars is installed if running outside the flow\n",
        "    try:\n",
        "        import polars as pl\n",
        "    except ImportError:\n",
        "        os.system('pip install polars matplotlib')\n",
        "        import polars as pl\n",
        "\n",
        "    generate_strategic_report()"
      ],
      "metadata": {
        "id": "kvk6uVFQtegb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}